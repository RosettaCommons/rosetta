#!/usr/bin/env python
# :noTabs=true:
# (c) Copyright Rosetta Commons Member Institutions.
# (c) This file is part of the Rosetta software suite and is made available under license.
# (c) The Rosetta software is developed by the contributing members of the Rosetta Commons.
# (c) For more information, see http://www.rosettacommons.org. Questions about this can be
# (c) addressed to University of Washington CoMotion, email: license@uw.edu.

from shutil import rmtree, copy
from os import path, makedirs, getcwd
import glob, re, subprocess, commands
from datetime import date
from optparse import OptionParser

class BaseSampleSource:

    def __init__(self):
        # These defaults should be either overwritten in the derived
        # class __init__ or set via the --sample_source_id
        # and--sample_source_description command line options

        sample_source_tag = getcwd().split("/")[-1]
        self.sample_source_id_default_value = \
            sample_source_tag + "_r%(svn_revision)s_%(date_code)s"

        self.sample_source_description_default_value = "SAMPLE SOURCE DESCRIPTION"


######################################################################
# options parsing functions
#
#
#####################################################################
    def initialize_options_parser(self):

        self.parser = OptionParser(usage="Usage: %prog [OPTIONS]")
        self.parser.add_option("--sample_source_id",
          default=self.sample_source_id_default_value,
          help="The output file is <output_dir>/<sample_source_id>/features_<sample_source_id>.db3. [default %default]")

        self.parser.add_option("--sample_source_description",
          default=self.sample_source_description_default_value,
          help="This is included into the features database, 'SELECT description from sample_sources;'")

        self.parser.add_option("--arguments",
          default="../../_arguments.py",
          help="A file that contains a python dictionary of variables initialized by the cluster.py scientific benchmark driver script usually generated by the test/scientific/cluster/cluster.py driver script and deposited into test/scientific/cluster/features/ . [default %default]")

    def parse_options(self, argv):

        (options, args) = self.parser.parse_args(args=argv)

        self.mvars = None
        try:
            f = open(options.arguments)
            self.mvars = eval(f.read())
            f.close()
        except:
            print "ERROR: The arguments file, %s, could no be loaded. This is usually the _arguments.py file generated by the cluster.py scientific benchmark script." % options.arguments

        self.mvars['date_code'] = date.today().strftime("%y%m%d")
        self.mvars['sample_source_path'] = getcwd()
        self.mvars['sample_source_id'] = options.sample_source_id % self.mvars
        self.mvars['sample_source_description'] = \
            options.sample_source_description % self.mvars

        return options


######################################################################
# mvar helper functions
#
#  The mvar variable contains all the configuration information to set
#  up the execution environment for a job. It is initialized by the
#  rosetta_tests/scientific/cluster/cluster.py driver script and then
#  added to durrning the job configuration process.
#
#
#####################################################################

    def apply_mvars(self, in_fname, out_fname):

        contents = file(in_fname).read()
        contents = contents % self.mvars

        f = file(out_fname, 'w')
        f.write(contents)
        f.close()

    def print_mvars(self):
        print "mvars:"
        for key, value in self.mvars.iteritems():
            print "\t%s\t'%s'" % (key.rjust(26), value)

#################################################################
#
#  The stepup process has the following steps:
#
#     1) Setup the input data
#     2) Setup the output paths
#     3) Setup the job files
#
#################################################################

    def setup(self):
        self.setup_input_data()

        self.print_mvars()

        self.setup_output_paths()
        self.setup_job_files()


    def setup_input_data(self):
        pass


    def setup_output_paths(self):
        output_path = "%(output_dir)s/%(sample_source_id)s" % self.mvars
        if path.exists(output_path):
            rmtree(output_path)
        try:
            makedirs(output_path)
        except OSError, e:
            print >>sys.stderr, "Cannot make directory:", e



    def setup_job_files(self):
        run_dir = "%(output_dir)s/%(sample_source_id)s" % self.mvars

        copy("%(workdir)s/sample_sources/merge.sh" % self.mvars,
             path.join(run_dir, "merge.sh"))

        for template_fname in glob.glob("*.TEMPLATE"):
            fname = template_fname[:-9]
            self.apply_mvars(template_fname, path.join(run_dir, fname))

#################################################################
#
# submit_<action_type>_job
#
#   These used to execuate the scientific benchmark on different platforms. Specify which action type when calling the cluster.py driver script
#
#   cd rosetta_tests/scientific/cluster
#   ./cluster.py [OPTIONS] submit_<action_type> features
#
#################################################################

    def submit_local_job(self):
        script_fname = "%(output_dir)s/%(sample_source_id)s/submit_local_job.sh" % self.mvars
        print "Submit local job %s..." % script_fname

        p = subprocess.Popen(["sh", script_fname])
        p.wait()


    def submit_lsf_job(self):
        script_fname = "%(output_dir)s/%(sample_source_id)s/submit_lsf_job.sh" % self.mvars
        print "Submit lsf job", script_fname, "to the %(lsf_queue_name)s queue requesting %(num_cores)s cores..." % self.mvars
        p = subprocess.Popen(["sh", script_fname])
        p.wait()

    def submit_condor_job(self):
        script_fname = "%(output_dir)s/%(sample_source_id)s/condor_submit_script" % self.mvars
        print "Submit condor job %s..." % script_fname

        o = commands.getoutput("which condor_submit")
        if o[:23] == 'which: no condor_submit':
            print """
ERROR: Attempting to submit job to condor cluster, but 'condor_submit' is not found.

 * If you intended to run it on a different cluster platform, use the
   --run-type flag. See:

      wiki.rosettacommons.org/index.php/FeaturesTutorialRunSciBench

   for available options.

 * If you did intend to run this on a condor cluster. Please check
   that the system configured approriately. 
"""
            return

        # Submit condor job and add condor_job_id to the global list of jobs daemon should wait for
        o = commands.getoutput('condor_submit %s' % script_fname)
	print 'Condor submit output:', o
        condor_job_id = int( o.split()[-1][:-1] )
        f = file('%(workdir)s/condor_job_ids' % self.mvars, 'a');  f.write(' %s ' % condor_job_id);  f.close()


    def submit_dryrun_job(self):

        print "Submit dryrun job..."
        import sqlite3

        log_fname = "%(output_dir)s/%(sample_source_id)s/features_local_%(sample_source_id)s.log" % self.mvars
        db_fname = "%(output_dir)s/%(sample_source_id)s/features_%(sample_source_id)s.db3" % self.mvars
        for i in range(int(self.mvars['num_cores'])):
            f = open( log_fname + "_" + str(i), 'w')
            f.write("log file %s" % i)
            f.close()

            conn =  sqlite3.connect(db_fname + "_" + str(i))
            conn.execute("CREATE TABLE structures (tag TEXT);")
            conn.execute("INSERT INTO structures (tag) VALUES ('struct_%s');" % str(i))
            conn.commit()
            conn.close()

    def submit(self):

        if self.mvars["run_type"] == "local": self.submit_local_job()
        elif self.mvars["run_type"] == "lsf": self.submit_lsf_job()
        elif self.mvars["run_type"] == "dryrun": self.submit_dryrun_job()
        elif self.mvars["run_type"] == "condor": self.submit_condor_job()
        else:
            print "ERROR: Unrecognized run_type: '%s'." % self.mvars["run_type"]
            print "ERROR: Recognized run_types for the submit action on the features scientific benchmark are ['local', 'lsf', 'dryrun', 'condor']."
